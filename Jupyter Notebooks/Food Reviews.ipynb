{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface\n",
    "In this notebook, I will try to use [Amazon food review data found on Kaggle](https://www.kaggle.com/snap/amazon-fine-food-reviews) in order to make a \n",
    "model that can predict whether or not a review is a postitive or negative review purely based on the text portion of the review. I will use multiple methods \n",
    "to build different models and compare their effectiveness with each other. This is my first time trying to use NLP methods so a lot of the things I do in this\n",
    "will most likely be learned along the way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "Since the dataset was found on Kaggle, there isn't much work needed to clean the data up for modeling, but I'd like to remove the columns I don't plan on using\n",
    "and separate positive reviews from negative ones. For simplicity, scores of 3 or higher are considered positive while scores of 1 and 2 are considered negative.\n",
    "I could make scores of 3 be considered neutral, but adding a 3rd category would likely make the model less effective/more confusing since neutral review are\n",
    "likely to contain words used in both negative and positive statements depending on context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read downloaded data\n",
    "import pandas as pd\n",
    "reviews_df = pd.read_csv('C:\\\\Users\\\\beton\\\\Documents\\\\Personal Projects\\\\Food Review NLP\\\\Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568449</th>\n",
       "      <td>568450</td>\n",
       "      <td>B001EO7N10</td>\n",
       "      <td>A28KG5XORO54AY</td>\n",
       "      <td>Lettie D. Carter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1299628800</td>\n",
       "      <td>Will not do without</td>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568450</th>\n",
       "      <td>568451</td>\n",
       "      <td>B003S1WTCU</td>\n",
       "      <td>A3I8AFVPEE8KI5</td>\n",
       "      <td>R. Sawyer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1331251200</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568451</th>\n",
       "      <td>568452</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A121AA1GQV751Z</td>\n",
       "      <td>pksd \"pk_007\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1329782400</td>\n",
       "      <td>Perfect for our maltipoo</td>\n",
       "      <td>These stars are small, so you can give 10-15 o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568452</th>\n",
       "      <td>568453</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A3IBEVCTXKNOH</td>\n",
       "      <td>Kathy A. Welch \"katwel\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1331596800</td>\n",
       "      <td>Favorite Training and reward treat</td>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568453</th>\n",
       "      <td>568454</td>\n",
       "      <td>B001LR2CU2</td>\n",
       "      <td>A3LGQPJCZVL9UC</td>\n",
       "      <td>srfell17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1338422400</td>\n",
       "      <td>Great Honey</td>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568454 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId                      ProfileName  \\\n",
       "0            1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1            2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2            3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3            4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4            5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "...        ...         ...             ...                              ...   \n",
       "568449  568450  B001EO7N10  A28KG5XORO54AY                 Lettie D. Carter   \n",
       "568450  568451  B003S1WTCU  A3I8AFVPEE8KI5                        R. Sawyer   \n",
       "568451  568452  B004I613EE  A121AA1GQV751Z                    pksd \"pk_007\"   \n",
       "568452  568453  B004I613EE   A3IBEVCTXKNOH          Kathy A. Welch \"katwel\"   \n",
       "568453  568454  B001LR2CU2  A3LGQPJCZVL9UC                         srfell17   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                          1                       1      5  1303862400   \n",
       "1                          0                       0      1  1346976000   \n",
       "2                          1                       1      4  1219017600   \n",
       "3                          3                       3      2  1307923200   \n",
       "4                          0                       0      5  1350777600   \n",
       "...                      ...                     ...    ...         ...   \n",
       "568449                     0                       0      5  1299628800   \n",
       "568450                     0                       0      2  1331251200   \n",
       "568451                     2                       2      5  1329782400   \n",
       "568452                     1                       1      5  1331596800   \n",
       "568453                     0                       0      5  1338422400   \n",
       "\n",
       "                                   Summary  \\\n",
       "0                    Good Quality Dog Food   \n",
       "1                        Not as Advertised   \n",
       "2                    \"Delight\" says it all   \n",
       "3                           Cough Medicine   \n",
       "4                              Great taffy   \n",
       "...                                    ...   \n",
       "568449                 Will not do without   \n",
       "568450                        disappointed   \n",
       "568451            Perfect for our maltipoo   \n",
       "568452  Favorite Training and reward treat   \n",
       "568453                         Great Honey   \n",
       "\n",
       "                                                     Text  \n",
       "0       I have bought several of the Vitality canned d...  \n",
       "1       Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2       This is a confection that has been around a fe...  \n",
       "3       If you are looking for the secret ingredient i...  \n",
       "4       Great taffy at a great price.  There was a wid...  \n",
       "...                                                   ...  \n",
       "568449  Great for sesame chicken..this is a good if no...  \n",
       "568450  I'm disappointed with the flavor. The chocolat...  \n",
       "568451  These stars are small, so you can give 10-15 o...  \n",
       "568452  These are the BEST treats for training and rew...  \n",
       "568453  I am very satisfied ,product is as advertised,...  \n",
       "\n",
       "[568454 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# remove unnecessary columns\n",
    "review_text = reviews_df.drop(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Time', 'Summary'], axis=1)\n",
    "\n",
    "# change scores to positive or negative label\n",
    "review_text.replace({'Score': {1: 'negative', 2: 'negative', 3: 'positive', 4: 'positive', 5: 'positive'}}, inplace = True)\n",
    "\n",
    "# make a trimmed version of the dataset for faster training (needed for SVM model otherwise it would take way too long)\n",
    "np.random.seed(10)\n",
    "remove_n = 558454\n",
    "drop_indices = np.random.choice(review_text.index, remove_n, replace=False)\n",
    "review_trimmed = review_text.drop(drop_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>I bought these for my husband who is currently...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>I have lived out of the US for over 7 yrs now,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>Natural Balance Dry Dog Food Lamb Meal and Bro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>not what I was expecting in terms of the compa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>Terrible! Artificial lemon taste, like Pledge ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Score                                               Text\n",
       "0  positive  I bought these for my husband who is currently...\n",
       "1  positive  I have lived out of the US for over 7 yrs now,...\n",
       "2  positive  Natural Balance Dry Dog Food Lamb Meal and Bro...\n",
       "3  positive  not what I was expecting in terms of the compa...\n",
       "4  negative  Terrible! Artificial lemon taste, like Pledge ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_trimmed.reset_index(drop=True, inplace=True)\n",
    "review_trimmed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I bought these for my husband who is currently overseas. He loves these, and apparently his staff likes them also.<br />There are generous amounts of Twizzlers in each 16-ounce bag, and this was well worth the price. <a href=\"http://www.amazon.com/gp/product/B001GVISJM\">Twizzlers, Strawberry, 16-Ounce Bags (Pack of 6)</a>\n"
     ]
    }
   ],
   "source": [
    "print(review_trimmed.iloc[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sir Kensington's did a great job of updating the classic ketchup with this wonderful product. The refreshed taste of this Ketchup is a great update, and now leaves me disappointed when I'm given Heinz while out at a restaurant.<br /><br />For you Heinz die hard fans out there, this is not the ketchup for you. But for those of you who wish you always knew what ketchup could be without the chemical aftertaste found in Heinz, be sure to give this Ketchup a try.<br /><br />Don't forget the spiced variety. Purchasing the pack with both the classic and spiced variety for your first Sir Kensington experience is definitely the way to go.\n"
     ]
    }
   ],
   "source": [
    "print(review_trimmed.iloc[6,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Simple Classifier using sklearn CountVectorizer and an SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into testing and training sets; training set is made small due to how long it takes for an svm model to be trained on large datasets with high dimensionality\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(review_trimmed['Text']), np.array(review_trimmed['Score']), test_size=0.30, random_state=1)\n",
    "\n",
    "# transform data\n",
    "vectorizer = CountVectorizer()\n",
    "x_train_vector = vectorizer.fit_transform(x_train)\n",
    "x_test_vector = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.SVC(kernel='linear')\n",
    "clf_svm.fit(x_train_vector, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8716666666666667"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate model\n",
    "clf_svm.score(x_test_vector, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the test set, the SVM model has an accuracy of about 87%, which is not bad. However, it's a good idea to test the model with a few made up inputs as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'positive', 'negative', 'positive'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ['This food is amazing! Tastes great!','Not good. The food was lacking in flavor', 'Tasted disgusting','The snacks could have been better, would not recommend.']\n",
    "X = vectorizer.transform(test)\n",
    "clf_svm.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the test statements above, we can see that the model has a problem with correctly identifying negative sentiments from phrases including multiple words. That's because this model only counts single words when vectorizing \n",
    "and which ignores the effects of modifying words like 'not'. I'll try to account for this using the ngram_range parameter for the CountVectorizer. I'll also see if using the stop_words parameter for further refinement may work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram 2 accuracy= 0.897\n",
      "ngram 2 with stop words accuracy= 0.8883333333333333\n",
      "stop words accuracy= 0.868\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate model with ngram range of 2\n",
    "vectorizer2 = CountVectorizer(ngram_range=(1,2))\n",
    "x_train_vector2 = vectorizer2.fit_transform(x_train)\n",
    "x_test_vector2 = vectorizer2.transform(x_test)\n",
    "\n",
    "clf_svm2 = svm.SVC(kernel='linear')\n",
    "clf_svm2.fit(x_train_vector2, y_train)\n",
    "\n",
    "print('ngram 2 accuracy=', clf_svm2.score(x_test_vector2, y_test))\n",
    "      \n",
    "# train and evaluate model with ngram range of 2 and stop words\n",
    "vectorizer3 = CountVectorizer(ngram_range=(1,2), stop_words='english')\n",
    "x_train_vector3 = vectorizer3.fit_transform(x_train)\n",
    "x_test_vector3 = vectorizer3.transform(x_test)\n",
    "\n",
    "clf_svm3 = svm.SVC(kernel='linear')\n",
    "clf_svm3.fit(x_train_vector3, y_train)\n",
    "      \n",
    "print('ngram 2 with stop words accuracy=', clf_svm3.score(x_test_vector3, y_test))\n",
    "\n",
    "# train and evaluate model with only stop_words accounted for\n",
    "vectorizer4 = CountVectorizer(stop_words='english')\n",
    "x_train_vector4 = vectorizer4.fit_transform(x_train)\n",
    "x_test_vector4 = vectorizer4.transform(x_test)\n",
    "\n",
    "clf_svm4 = svm.SVC(kernel='linear')\n",
    "clf_svm4.fit(x_train_vector4, y_train)\n",
    "      \n",
    "print('stop words accuracy=', clf_svm4.score(x_test_vector4, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above, it seems that altering the word combination count increased model accuracy, but removing stop words decreased accuracy. The default 'english' stop words list\n",
    "is known to have issues, so using an alternative list found online or making my own list of stop words may work better. To further optimize the model, I could iterate to find the best-scoring\n",
    "combination of ngram values and stop word lists. \n",
    "\n",
    "I could also use a different vectorizer to further fit and transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfTransformer score= 0.8893333333333333\n"
     ]
    }
   ],
   "source": [
    "# Make a model using TfidfTransformer based on the original model\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer()\n",
    "x_train_tfidf = tf_transformer.fit_transform(x_train_vector)\n",
    "x_test_tfidf = tf_transformer.fit_transform(x_test_vector)\n",
    "\n",
    "# Train model\n",
    "clf_svm_tfidf = svm.SVC(kernel='linear')\n",
    "clf_svm_tfidf.fit(x_train_tfidf, y_train)\n",
    "print('TfidfTransformer score=', clf_svm_tfidf.score(x_test_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TfidfTransformer, which weighs the frequency of tokens in order to better determine how important they are, also improved the score from the original model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Simple Classifier using sklearn CountVectorizer and a Naive Bayes Model\n",
    "\n",
    "Now I'll do the same thing using a Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.879"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a pipeline to simplify the fitting and modeling process\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "NB_pipeline = Pipeline([('vect', CountVectorizer()),('clf', MultinomialNB())])\n",
    "\n",
    "# train and test model using the same training and testing sets\n",
    "NB_pipeline.fit(x_train, y_train)\n",
    "\n",
    "# evaluate model\n",
    "NB_pipeline.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and evaluate model with ngram range of 2\n",
    "NB_pipeline2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),('clf', MultinomialNB())])\n",
    "NB_pipeline2.fit(x_train, y_train)\n",
    "\n",
    "print('NB ngram 2 accuracy=', NB_pipeline2.score(x_test, y_test))\n",
    "      \n",
    "# train and evaluate model with ngram range of 2 and stop words\n",
    "NB_pipeline3 = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), stop_words='english')),('clf', MultinomialNB())])\n",
    "NB_pipeline3.fit(x_train, y_train)\n",
    "      \n",
    "print('NB ngram 2 with stop words accuracy=', NB_pipeline3.score(x_test, y_test))\n",
    "\n",
    "# train and evaluate model with only stop_words accounted for\n",
    "NB_pipeline4 = Pipeline([('vect', CountVectorizer(stop_words='english')),('clf', MultinomialNB())])\n",
    "NB_pipeline4.fit(x_train, y_train)\n",
    "      \n",
    "print('NB stop words accuracy=', NB_pipeline4.score(x_test, y_test))\n",
    "\n",
    "# Make a model using TfidfTransformer\n",
    "NB_tfidf_pipeline = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', MultinomialNB())])\n",
    "NB_tfidf_pipeline.fit(x_train,y_train)\n",
    "\n",
    "print('NB tfidf=', NB_tfidf_pipeline.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Naive Bayes model, the model that implements only stop words performs the best, though not quite as well as the most optimized SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Classifier using Spacy Word Vectors and an SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained spacy pipeline for English\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# apply pipeline to training data and store Doc\n",
    "docs = [nlp(text) for text in x_train]\n",
    "x_train_wv = [x.vector for x in docs]\n",
    "\n",
    "docs2 = [nlp(text) for text in x_test]\n",
    "x_test_wv = [x.vector for x in docs2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm_wv = svm.SVC(kernel='linear')\n",
    "clf_svm_wv.fit(x_train_wv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM using spacy accuracy= 0.8733333333333333\n"
     ]
    }
   ],
   "source": [
    "print('SVM using spacy accuracy=', clf_svm_wv.score(x_test_wv, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Spacy, this SVM model performs roughly the same as the original SVM model using CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up Text Using Regex\n",
    "\n",
    "Some of the text reviews contain html tags which end up being used to train the model despite having no meaning.\n",
    "Using regular expression, I'll try to get rid of the main tags (line breaks and hyperlinks) that I found here and\n",
    "there when skimming though the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "review_trimmed.reset_index()\n",
    "for i in range(len(review_trimmed.index)):\n",
    "    text = review_trimmed.iloc[i]['Text']\n",
    "    regexp = re.compile(r\"\\<a.*a\\>|\\<br.*/>\")\n",
    "    text2 = re.sub(regexp, '', text)\n",
    "    review_trimmed.at[i,'Text'] = text2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Lemmatization and Stop Word Removal to Clean Data Further\n",
    "\n",
    "With some of the extraneous tags gone, I can do some more cleaning. The main changes I'll make to the texts are lemmatization and stop word removal\n",
    "(using nltk this time). (*I originally wanted to use spell check with TextBlob but using spell check on long statements in a loop of 10000 statements\n",
    "took far too long)*. Stop word removal and spell correction are relatively self-explanatory. Lemmatization is the process of grouping together the \n",
    "different forms a single word can have into one group so that they can be analyzed as a single entity. Words grouped up through lemmatization should \n",
    "allow NLP models to be better trained since they should, ideally, no long treat different forms of the same word as different words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\beton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\beton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\beton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use nltk lemmatizer to lemmatize \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "review_cleaned = pd.DataFrame()\n",
    "\n",
    "\n",
    "for i in range(len(review_trimmed.index)):\n",
    "    text = review_trimmed.iloc[i]['Text']\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # make text lower case\n",
    "    words = [w.lower() for w in words]    \n",
    "\n",
    "    #remove puncuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    words_stripped = [word.translate(table) for word in words]\n",
    "    \n",
    "    # remove non-alpha words\n",
    "    words_no_punct = [word for word in words_stripped if word.isalpha()]\n",
    "    \n",
    "    # filter out stop words\n",
    "    words_cleaned = [word for word in words_no_punct if not word in stop_words]\n",
    "    \n",
    "    new_text = \" \".join(words_cleaned)\n",
    "    review_cleaned.at[i,'Score'] = review_trimmed.iloc[i]['Score']\n",
    "    review_cleaned.at[i,'Text'] = new_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.877"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data into testing and training sets; training set is made small due to how long it takes for an svm model to be trained on large datasets with high dimensionality\n",
    "x_clean_train, x_clean_test, y_clean_train, y_clean_test = train_test_split(np.array(review_cleaned['Text']), np.array(review_cleaned['Score']), test_size=0.2, random_state=1)\n",
    "\n",
    "# transform data\n",
    "vectorizer = CountVectorizer()\n",
    "x_clean_train_vector = vectorizer.fit_transform(x_clean_train)\n",
    "x_clean_test_vector = vectorizer.transform(x_clean_test)\n",
    "\n",
    "clf_svm = svm.SVC(kernel='linear')\n",
    "clf_svm.fit(x_clean_train_vector, y_clean_train)\n",
    "\n",
    "clf_svm.score(x_clean_test_vector, y_clean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram 2 accuracy= 0.8845\n",
      "ngram 2 with stop words accuracy= 0.885\n",
      "stop words accuracy= 0.8665\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate model with ngram range of 2\n",
    "vectorizer2 = CountVectorizer(ngram_range=(1,2))\n",
    "x_train_vector2 = vectorizer2.fit_transform(x_clean_train)\n",
    "x_test_vector2 = vectorizer2.transform(x_clean_test)\n",
    "\n",
    "clf_svm2 = svm.SVC(kernel='linear')\n",
    "clf_svm2.fit(x_train_vector2, y_clean_train)\n",
    "\n",
    "print('ngram 2 accuracy=', clf_svm2.score(x_test_vector2, y_clean_test))\n",
    "      \n",
    "# train and evaluate model with ngram range of 2 and stop words\n",
    "vectorizer3 = CountVectorizer(ngram_range=(1,2), stop_words='english')\n",
    "x_train_vector3 = vectorizer3.fit_transform(x_clean_train)\n",
    "x_test_vector3 = vectorizer3.transform(x_clean_test)\n",
    "\n",
    "clf_svm3 = svm.SVC(kernel='linear')\n",
    "clf_svm3.fit(x_train_vector3, y_clean_train)\n",
    "      \n",
    "print('ngram 2 with stop words accuracy=', clf_svm3.score(x_test_vector3, y_clean_test))\n",
    "\n",
    "# train and evaluate model with only stop_words accounted for\n",
    "vectorizer4 = CountVectorizer(stop_words='english')\n",
    "x_train_vector4 = vectorizer4.fit_transform(x_clean_train)\n",
    "x_test_vector4 = vectorizer4.transform(x_clean_test)\n",
    "\n",
    "clf_svm4 = svm.SVC(kernel='linear')\n",
    "clf_svm4.fit(x_train_vector4, y_clean_train)\n",
    "      \n",
    "print('stop words accuracy=', clf_svm4.score(x_test_vector4, y_clean_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing classifier accuracy using the default settings, the SVM model using the cleaned data performed slightly better than the original SVM model, but when using differnt ngram and stop_words parameters, the model performs worse than the previous SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
